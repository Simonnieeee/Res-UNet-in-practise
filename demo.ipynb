{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is used to process data into .pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import imageio \n",
    "\n",
    "\n",
    "def generate_mask(bd_pts=None, img_rows=None, lesion=None):\n",
    "    \"\"\" Generate masks from boundary surfaces and lesion masks\n",
    "\n",
    "        Args:\n",
    "            bd_pts: boundary surfaces, shape = [bds, img_cols]\n",
    "            lesion: lesion masks, shape =  [img_rows, img_cols] \n",
    "\n",
    "        Returns:\n",
    "            label: training labels, shape = (img_rows, img_cols)\n",
    "\n",
    "    \"\"\"\n",
    "    bds,img_cols = bd_pts.shape\n",
    "    if img_rows is None:\n",
    "        assert lesion is not None\n",
    "        img_rows = lesion.shape[0]\n",
    "    label = np.zeros((img_rows,img_cols))*np.nan\n",
    "    for j in range(img_cols):\n",
    "        \n",
    "        cols = np.arange(img_rows)\n",
    "        index = cols - bd_pts[0,j] < 0\n",
    "        label[index,j] = 0\n",
    "        index = cols - bd_pts[bds-1,j] >= 0\n",
    "        label[index,j] = bds\n",
    "        for k in range(bds-1):\n",
    "            index_up = cols - bd_pts[k,j] >= 0\n",
    "            index_down = cols - bd_pts[k+1,j] < 0\n",
    "            label[index_up&index_down,j] = k+1\n",
    "    if lesion is not None:\n",
    "        label[lesion>0] = bds+1\n",
    "\n",
    "    return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3821066/1016019205.py:24: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = np.array(imageio.imread(str(image_pt),pilmode = 'L'))/255\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "labels_pt = \"data/cirrus/label\"\n",
    "images_pt = \"data/cirrus/image\"\n",
    "# labels_pt = \"data/spectralis/hcval/label\"\n",
    "# images_pt = \"data/spectralis/hcval/image\"\n",
    "\n",
    "## read image\n",
    "labels_files = os.listdir(labels_pt)\n",
    "labels_files = sorted(labels_files)\n",
    "images_files = os.listdir(images_pt)\n",
    "images_files = sorted(images_files)\n",
    "\n",
    "file_names = [x.split('.')[0] for x in labels_files] ## list\n",
    "\n",
    "print(len(labels_files),len(images_files))\n",
    "\n",
    "images_data_list = []\n",
    "labels_data_list = []\n",
    "for image_file,label_file in zip(images_files,labels_files):\n",
    "    image_pt = os.path.join(images_pt,image_file)\n",
    "    label_pt = os.path.join(labels_pt,label_file)\n",
    "\n",
    "    image = np.array(imageio.imread(str(image_pt),pilmode = 'L'))/255\n",
    "    images_data_list.append(image)\n",
    "    with open(str(label_pt),'r') as f:\n",
    "        dicts = json.loads(f.read())\n",
    "\n",
    "    if 'lesion' in dicts.keys():\n",
    "        mask = np.array(dicts['lesion'])\n",
    "        mask[mask>1] = 1\n",
    "    else:    \n",
    "        mask = np.zeros(image.shape)\n",
    "\n",
    "    bds = np.array(dicts['bds'], dtype=float) - 1 \n",
    "    label = generate_mask(bd_pts=bds,lesion=mask)\n",
    "    labels_data_list.append(label)\n",
    "\n",
    "    image_file_name = image_file.split('.')[0]\n",
    "    label_file_name = label_file.split('.')[0]\n",
    "\n",
    "    if image_file_name != label_file_name:\n",
    "        print(\"image_file_name != label_file_name\")\n",
    "        print(image_file)\n",
    "        print(label_file)\n",
    "        break\n",
    "    assert image.shape == label.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "48\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "print(len(images_data_list))\n",
    "print(len(labels_data_list))\n",
    "print(len(file_names))\n",
    "images_data_list = np.array(images_data_list)\n",
    "labels_data_list = np.array(labels_data_list)\n",
    "\n",
    "data = {'images':images_data_list,'labels':labels_data_list, 'file_names':file_names}\n",
    "\n",
    "## save data\n",
    "f_name = 'data/cirrus/data.pkl'\n",
    "with open(f_name, 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "9.0\n"
     ]
    }
   ],
   "source": [
    "## load data\n",
    "with open(f_name, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "new_labels = data['labels']\n",
    "new_images = data['images']\n",
    "new_filenames = data['file_names']\n",
    "\n",
    "print(np.max(new_images[0]))\n",
    "print(np.max(new_labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II\n",
    "开始载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "f_val = \"data/spectralis/hcval/data.pkl\"\n",
    "f_train = \"data/spectralis/hctrain/data.pkl\"\n",
    "f_test = \"data/spectralis/hceval/data.pkl\"\n",
    "\n",
    "with open(f_val, 'rb') as f:\n",
    "    data_val = pickle.load(f)\n",
    "\n",
    "labels_val = data_val['labels']\n",
    "images_val = data_val['images']\n",
    "filenames_val = data_val['file_names']\n",
    "\n",
    "\n",
    "with open(f_train, 'rb') as f:\n",
    "    data_train = pickle.load(f)\n",
    "\n",
    "labels_train = data_train['labels']\n",
    "images_train = data_train['images']\n",
    "filenames_train = data_train['file_names']\n",
    "\n",
    "with open(f_test, 'rb') as f:\n",
    "    data_test = pickle.load(f)\n",
    "\n",
    "labels_test = data_test['labels']\n",
    "images_test = data_test['images']\n",
    "filenames_test = data_test['file_names']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "## 调整数据size\n",
    "images_train = images_train[:, np.newaxis, :, :]\n",
    "images_val = images_val[:, np.newaxis, :, :]\n",
    "labels_train = labels_train[:, np.newaxis, :, :]\n",
    "labels_val = labels_val[:, np.newaxis, :, :]\n",
    "images_test = images_test[:, np.newaxis, :, :]\n",
    "labels_test = labels_test[:, np.newaxis, :, :]\n",
    "\n",
    "## 转化为torch tensor\n",
    "labels_train = torch.tensor(labels_train, dtype=torch.float32)\n",
    "labels_val = torch.tensor(labels_val, dtype=torch.float32)\n",
    "images_train = torch.tensor(images_train, dtype=torch.float32)\n",
    "images_val = torch.tensor(images_val, dtype=torch.float32)\n",
    "labels_test = torch.tensor(labels_test, dtype=torch.float32)\n",
    "images_test = torch.tensor(images_test, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([588, 1, 128, 1024])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [9., 9., 9.,  ..., 9., 9., 9.],\n",
      "         [9., 9., 9.,  ..., 9., 9., 9.],\n",
      "         [9., 9., 9.,  ..., 9., 9., 9.]]])\n",
      "74 19\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 设置超参数\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_CLASSES = 10\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "## label转化为one-hot\n",
    "print(labels_train.shape)\n",
    "\n",
    "def to_one_hot(labels_train):\n",
    "    num, _, h, w = labels_train.size()\n",
    "    labels_train_flat = labels_train.view(-1).long()\n",
    "    one_hot_labels = F.one_hot(labels_train_flat, num_classes=NUM_CLASSES)\n",
    "    one_hot_labels = one_hot_labels.view(num, NUM_CLASSES, h, w)\n",
    "    return one_hot_labels.float()\n",
    "\n",
    "labels_train = to_one_hot(labels_train)\n",
    "labels_val = to_one_hot(labels_val)\n",
    "labels_test = to_one_hot(labels_test)\n",
    "\n",
    "print(labels_train.shape)\n",
    "print(labels_val[0,:,0,0])\n",
    "\n",
    "## 创建dataset和加载器\n",
    "train_dataset = TensorDataset(images_train, labels_train)\n",
    "val_dataset = TensorDataset(images_val, labels_val)\n",
    "test_dataset = TensorDataset(images_test, labels_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "print(len(train_loader), len(val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n",
      "torch.Size([8, 10, 128, 1024]) torch.Size([8, 10, 128, 1024])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m bst_model_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m---> 43\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     losses_train\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, criterion, data_loader, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 17\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_loader)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ResUnet import *\n",
    "\n",
    "def train_one_epoch(model, optimizer, criterion, data_loader, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        print(outputs.shape, labels.shape)\n",
    "        # outputs = \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    return train_loss / len(data_loader)\n",
    "\n",
    "def validate(model, criterion, data_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(data_loader)\n",
    "\n",
    "output_dir = \"output\"\n",
    "\n",
    "model = ResUnet(1, 10).to(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "losses_train = []\n",
    "bst_model_num = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_one_epoch(model, optimizer, criterion, train_loader, DEVICE)\n",
    "    losses_train.append(train_loss)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        val_loss = validate(model, criterion, val_loader, DEVICE)\n",
    "        torch.save(model.state_dict(), output_dir+ '/checkpoints/model_at_epoch_{:02d}.pth'.format(epoch+1))\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            bst_model_num = epoch+1\n",
    "    print(f'Epoch: {epoch+1:03d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "print(\"Best model at epoch: \", bst_model_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1024)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(images_data[0].shape)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(labels_data[0], cmap='gray')\n",
    "# plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selfDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
